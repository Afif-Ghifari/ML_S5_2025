{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ffe1e1",
   "metadata": {},
   "source": [
    "<h1>JS13 - Artificial Neural Network (ANN) dan Evaluasi Classifier</h1>\n",
    "\n",
    "<b>Nama : Muhammad Afif Al Ghifari</b><br>\n",
    "<b>Kelas: TI-3H</b><br>\n",
    "<b>NIM  : 2341720168</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874e331",
   "metadata": {},
   "source": [
    "## Praktikum 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca01a909",
   "metadata": {},
   "source": [
    "Langkah:\n",
    "\n",
    "1. Buat dataset sederhana (XOR).\n",
    "2. Inisialisasi bobot dan bias.\n",
    "3. Implementasikan forward pass.\n",
    "4. Hitung error dan lakukan backpropagation.\n",
    "5. Update bobot menggunakan gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf7084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.26662325238401663\n",
      "Epoch 1000, Loss: 0.24328461416333172\n",
      "Epoch 2000, Loss: 0.21335704334601674\n",
      "Epoch 3000, Loss: 0.17233998737445452\n",
      "Epoch 4000, Loss: 0.06679749195183388\n",
      "Epoch 5000, Loss: 0.025906772175867537\n",
      "Epoch 6000, Loss: 0.014607086457196291\n",
      "Epoch 7000, Loss: 0.009860912830580073\n",
      "Epoch 8000, Loss: 0.00734007081057488\n",
      "Epoch 9000, Loss: 0.005802213965115707\n",
      "Prediksi:\n",
      "[[0.06692621]\n",
      " [0.9267425 ]\n",
      " [0.9247376 ]\n",
      " [0.05996213]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Fungsi aktivasi\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Output akhir\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847526c9",
   "metadata": {},
   "source": [
    "Tugas 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833f7c9",
   "metadata": {},
   "source": [
    "- Ubah jumlah neuron hidden layer menjadi 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ed1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.25799784854672636\n",
      "Epoch 1000, Loss: 0.21192541641104767\n",
      "Epoch 2000, Loss: 0.14144565982897034\n",
      "Epoch 3000, Loss: 0.05878831196286855\n",
      "Epoch 4000, Loss: 0.02034936270507408\n",
      "Epoch 5000, Loss: 0.00996981253925723\n",
      "Epoch 6000, Loss: 0.00620236694630593\n",
      "Epoch 7000, Loss: 0.004389182557397747\n",
      "Epoch 8000, Loss: 0.003353171672753776\n",
      "Epoch 9000, Loss: 0.002692691871470751\n",
      "Prediksi:\n",
      "[[0.04171937]\n",
      " [0.95548195]\n",
      " [0.95568306]\n",
      " [0.05718971]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size = 3     # <-- diubah\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Fungsi aktivasi\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backprop\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Output akhir\n",
    "print(\"Prediksi:\")\n",
    "print(a2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24adf58",
   "metadata": {},
   "source": [
    "- Bandingkan hasil loss dengan konfigurasi awal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d54ab",
   "metadata": {},
   "source": [
    "**Jawab:**<br>\n",
    "Konfigurasi 2 menunjukkan performa yang lebih baik dibanding konfigurasi 1 karena menghasilkan loss yang lebih rendah dan menurun lebih cepat, menandakan model belajar lebih optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36460f8a",
   "metadata": {},
   "source": [
    "- Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4bf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.17865517228497224\n",
      "Epoch 1000, Loss: 0.0038608912751372363\n",
      "Epoch 2000, Loss: 0.0015800594028312841\n",
      "Epoch 3000, Loss: 0.0009612235435910112\n",
      "Epoch 4000, Loss: 0.0006821000628761248\n",
      "Epoch 5000, Loss: 0.0005252698894860234\n",
      "Epoch 6000, Loss: 0.0004254750919766378\n",
      "Epoch 7000, Loss: 0.0003566567148040721\n",
      "Epoch 8000, Loss: 0.00030644673272636254\n",
      "Epoch 9000, Loss: 0.00026829356750302804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = relu(z1)  # ReLU\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    error = y - a2\n",
    "\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(z1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
